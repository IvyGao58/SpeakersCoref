# Word embeddings.
glove_300d_filtered {
  path=../CorefBaselines/BasePronounCoref/glove_emb/glove.840B.300d.txt.filtered
  size=300
}

glove_300d_2w {
  path=../CorefBaselines/BasePronounCoref/glove_emb/glove_50_300_2.txt
  size=300
}

zhihu_300d_filtered {
  path=chinese_emb/sgns.zhihu.word.filtered
  size=300
}

cail_300d {
  path=chinese_emb/cail.vector
  size=300
}

renmin_300d {
  path=chinese_emb/sgns.renmin.bigram-char
  size=300
}

# Main configuration.
best {
GPU=1
  # Computation limits.
  max_top_antecedents=50
  max_training_sentences=50
  top_span_ratio=1

  # Model hyperparameters.
  filter_widths=[3, 4, 5]
  filter_size=50
  emb_filter_widths=[1, 2, 3]
  emb_filter_size=128
  char_embedding_size=8
  char_vocab_path=chinese_emb/char_vocab.chinese.txt
  context_embeddings=${zhihu_300d_filtered}
  head_embeddings=${renmin_300d}
  contextualization_size=200
  contextualization_layers=3
  ffnn_size=150
  ffnn_depth=2
  feature_size=20
  max_span_width=42
  use_metadata=true
  use_features=true
  use_multi_span=false
  model_heads=false
  coref_depth=2
  lm_layers=3
  lm_size=1024
  coarse_to_fine=true

  # knowledge_hyperparameters
  softmax_threshold=0.0001
  softmax_biaffine_threshold=0.001
  knowledge_as_feature=false
  order_as_features=true
  order_length=10
  use_elmo=true
  use_word_emb=true
  random_sample_training=true
  apply_knowledge=true
  softmax_pruning=true
  softmax_biaffine_pruning=true
  knowledge_pruning=true
  attention=true
  number=true
  plurality=false
  type=true
  nsubj= false
  dobj=false
  predict=false
  apply_biaffine=true
  use_word_attn=true
  use_PMS_attention=false
  use_PMS_gcn=true
  use_xor_matrix=false
  use_ms_pair=false

  # Learning hyperparameters.
  max_gradient_norm=5.0
  lstm_dropout_rate=0.2
  lexical_dropout_rate=0.2
  dropout_rate=0.2
  optimizer=adam
  learning_rate=0.001
  decay_rate=0.999
  decay_frequency=100
  size_per_head=256
  size_per_head_with_gcn=820
  biaffine_out_features=2
  biaffine_score_weight = 0.5

  # Other.
  train_path=data/train.law.jsonlines
  eval_path=data/dev.law.jsonlines
  test_path=data/test.law.jsonlines
  lm_path=elmo_cache.hdf5
  title_map_path=data/title_map.txt
  predict_title_map_path=data/title_map.txt
  genres=["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency=900
  eval_matrix_path=eval_matrix.txt
  report_frequency=100
  k_fold=5
  log_root=logs
  max_patience=20
}

p_m_coref {
GPU=0
  # Computation limits.
  max_top_antecedents=50
  max_training_sentences=50
  top_span_ratio=1

  # Model hyperparameters.
  filter_widths=[3, 4, 5]
  filter_size=50
  emb_filter_widths=[1, 2, 3]
  emb_filter_size=128
  char_embedding_size=8
  char_vocab_path="../../../disk0/gaojun/SpeakersCoref/chinese_emb/char_vocab.chinese.txt"
  context_embeddings=${zhihu_300d_filtered}
  head_embeddings=${renmin_300d}
  contextualization_size=200
  contextualization_layers=3
  ffnn_size=150
  ffnn_depth=2
  feature_size=20
  max_span_width=42
  use_metadata=true
  use_features=true
  use_multi_span=false
  model_heads=false
  coref_depth=2
  lm_layers=3
  lm_size=1024
  coarse_to_fine=true

  # knowledge_hyperparameters
  # softmax_threshold=0.000001
  softmax_threshold=0.0001
  softmax_biaffine_threshold=0.001
  knowledge_as_feature=false
  order_as_features=true
  order_length=10
  use_elmo=true
  random_sample_training=true
  apply_knowledge=true
  apply_biaffine=true
  use_word_attn=true
  softmax_pruning=true
  softmax_biaffine_pruning=true
  knowledge_pruning=true
  attention=true
  number=true
  plurality=false
  type=true
  nsubj= false
  dobj=false
  predict=false
  use_PMS_attention=false
  use_MS_gcn=true

  # Learning hyperparameters.
  max_gradient_norm=5.0
  lstm_dropout_rate=0.2
  lexical_dropout_rate=0.2
  dropout_rate=0.2
  softmax_threashold=0.2
  optimizer=adam
  learning_rate=0.001
  decay_rate=0.999
  decay_frequency=100
  size_per_head=256
  biaffine_out_features=2
  biaffine_score_weight = 0.5

  # Other.
  train_path=data/law/p2n/train.jsonlines
  eval_path=data/law/p2n/dev.jsonlines
  test_path=data/law/p2n/test.jsonlines
  lm_path=../../../disk0/gaojun/SpeakersCoref/elmoformanylangs/elmo_chinese_cache_v2.hdf5
  title_map_path=data/law/title_map.txt
  predict_title_map_path=data/law/predict_title_map.txt
  genres=["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency=900
  eval_matrix_path=eval_matrix.txt
  report_frequency=100
  k_fold=5
  log_root=logs
  max_patience=20
}


p_s_coref {
GPU=0
  # Computation limits.
  max_top_antecedents=50
  max_training_sentences=50
  top_span_ratio=1

  # Model hyperparameters.
  filter_widths=[3, 4, 5]
  filter_size=50
  emb_filter_widths=[1, 2, 3]
  emb_filter_size=128
  char_embedding_size=8
  char_vocab_path="chinese_emb/char_vocab.chinese.txt"
  context_embeddings=${zhihu_300d_filtered}
  head_embeddings=${renmin_300d}
  contextualization_size=200
  contextualization_layers=3
  ffnn_size=150
  ffnn_depth=2
  feature_size=20
  max_span_width=30
  use_metadata=true
  use_features=true
  use_multi_span=false
  model_heads=false
  coref_depth=2
  lm_layers=3
  lm_size=1024
  coarse_to_fine=true

  # knowledge_hyperparameters
  # softmax_threshold=0.000001
  softmax_threshold=0.0001
  softmax_biaffine_threshold=0.000001
  knowledge_as_feature=false
  order_as_features=false
  order_length=10
  use_elmo=true
  random_sample_training=true
  apply_knowledge=true
  apply_biaffine=true
  softmax_pruning=true
  softmax_biaffine_pruning=true
  knowledge_pruning=true
  attention=true
  number=true
  plurality=false
  type=true
  nsubj= false
  dobj=false
  predict=false
  use_PMS_attention=false
  use_MS_gcn=true

  # Learning hyperparameters.
  max_gradient_norm=5.0
  lstm_dropout_rate=0.2
  lexical_dropout_rate=0.2
  dropout_rate=0.2
  softmax_threashold=0.2
  optimizer=adam
  learning_rate=0.001
  decay_rate=0.999
  decay_frequency=100
  size_per_head=256
  biaffine_out_features=2
  biaffine_score_weight = 0.7

  # Other.
  train_path=data/law/train.p2s.jsonlines
  eval_path=data/law/dev.p2s.jsonlines
  test_path=data/law/test.p2s.jsonlines
  lm_path=elmoformanylangs/elmo_chinese_cache_p2s.hdf5
  title_map_path=data/law/title_map.txt
  predict_title_map_path=data/law/predict_title_map.txt
  genres=["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency=900
  eval_matrix_path=eval_matrix.txt
  report_frequency=100
  k_fold=5
  log_root=logs
  max_patience=20
}

add_gcn_coref {
GPU=1
  # Computation limits.
  max_top_antecedents=50
  max_training_sentences=50
  top_span_ratio=1

  # Model hyperparameters.
  filter_widths=[3, 4, 5]
  filter_size=50
  emb_filter_widths=[1, 2, 3]
  emb_filter_size=128
  char_embedding_size=8
  char_vocab_path=chinese_emb/char_vocab.chinese.txt
  context_embeddings=${zhihu_300d_filtered}
  # context_embeddings=${cail_300d}
  head_embeddings=${renmin_300d}
  contextualization_size=200
  contextualization_layers=3
  ffnn_size=150
  ffnn_depth=2
  feature_size=20
  max_span_width=42
  use_metadata=true
  use_features=true
  use_multi_span=false
  model_heads=false
  coref_depth=2
  lm_layers=3
  lm_size=1024
  coarse_to_fine=true

  # knowledge_hyperparameters
  # softmax_threshold=0.000001
  softmax_threshold=0.0001
  softmax_biaffine_threshold=0.001
  knowledge_as_feature=false
  order_as_features=true
  order_length=10
  use_elmo=true
  use_word_emb=true
  random_sample_training=true
  apply_knowledge=true
  softmax_pruning=true
  softmax_biaffine_pruning=true
  knowledge_pruning=true
  attention=true
  number=true
  plurality=false
  type=true
  nsubj= false
  dobj=false
  predict=false
  apply_biaffine=true
  use_word_attn=true
  use_PMS_attention=false
  use_PMS_gcn=true
  use_xor_matrix=false
  use_ms_pair=false

  # Learning hyperparameters.
  max_gradient_norm=5.0
  lstm_dropout_rate=0.2
  lexical_dropout_rate=0.2
  dropout_rate=0.2
  optimizer=adam
  learning_rate=0.001
  decay_rate=0.999
  decay_frequency=100
  size_per_head=256
  size_per_head_with_gcn=820
  biaffine_out_features=2
  biaffine_score_weight = 0.5

  # Other.
  train_path=data/law/p2sHint2/train.p2sSortHint2.jsonlines
  eval_path=data/law/p2sHint2/dev.p2sSortHint2.jsonlines
  test_path=data/law/p2sHint2/test.p2sSortHint2.jsonlines
  lm_path=elmoformanylangs/elmo_chinese_cache_p2sHint1.hdf5
  title_map_path=data/law/p2sHint2/p2s_hint2_title_map.txt
  predict_title_map_path=data/law/predict_title_map.txt
  genres=["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency=900
  eval_matrix_path=eval_matrix.txt
  report_frequency=100
  k_fold=5
  log_root=logs
  max_patience=20
}

conll_coref {
GPU=1
  # Computation limits.
  max_top_antecedents=50
  max_training_sentences=50
  top_span_ratio=1

  # Model hyperparameters.
  filter_widths=[3, 4, 5]
  filter_size=50
  emb_filter_widths=[1, 2, 3]
  emb_filter_size=128
  char_embedding_size=8
  char_vocab_path=../CorefBaselines/BasePronounCoref/glove_emb/char_vocab.english.txt
  context_embeddings=${glove_300d_filtered}
  head_embeddings=${glove_300d_2w}
  contextualization_size=200
  contextualization_layers=3
  ffnn_size=150
  ffnn_depth=2
  feature_size=20
  max_span_width=30
  use_metadata=true
  use_features=true
  use_multi_span=false
  model_heads=false
  coref_depth=2
  lm_layers=3
  lm_size=1024
  coarse_to_fine=true

  # knowledge_hyperparameters
  # softmax_threshold=0.000001
  softmax_threshold=0.000001
  softmax_biaffine_threshold=0.001
  knowledge_as_feature=false
  order_as_features=false
  order_length=10
  use_elmo=true
  use_word_emb=true
  random_sample_training=true
  apply_knowledge=true
  softmax_pruning=true
  softmax_biaffine_pruning=true
  knowledge_pruning=true
  attention=true
  number=true
  plurality=false
  type=true
  nsubj= false
  dobj=false
  predict=false
  use_word_attn=true
  use_PMS_attention=false

  # Learning hyperparameters.
  max_gradient_norm=5.0
  lstm_dropout_rate=0.4
  lexical_dropout_rate=0.5
  dropout_rate=0.2
  optimizer=adam
  learning_rate=0.001
  decay_rate=0.999
  decay_frequency=100
  size_per_head=256
  size_per_head_with_gcn=820
  biaffine_out_features=2
  biaffine_score_weight = 0.5

  # Bert parameters
  use_bert = true
  bert_size = 768
  max_seq_length = 512
  bert_config = chinese_bert_768/bert_config.json
  vocab_file = chinese_bert_768/vocab.txt
  init_checkpoint = chinese_bert_768/bert_model.ckpt
  do_lower_case = true


  # Other.
  train_path=data/law/conll/sample/train.english.jsonlines
  eval_path=data/law/conll/sample/dev.english.jsonlines
  test_path=data/law/conll/sample/test.english.jsonlines
  lm_path=/home/gaojun/CorefBaselines/BasePronounCoref/elmo_cache.hdf5
  genres=["bc", "bn", "mz", "nw", "pt", "tc", "wb"]
  eval_frequency=1000
  eval_matrix_path=eval_matrix.txt
  report_frequency=100
  k_fold=5
  log_root=logs
  max_patience=20
}

